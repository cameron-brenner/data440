# -*- coding: utf-8 -*-
"""hw3prob2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wccJMgFDGrCSDu7CBJeafDujb5Wr-g7M
"""

import numpy as np
from matplotlib import pyplot as plt
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import KFold

def genDataSet(N):
    X = np.random.normal(0, 1, N)
    ytrue = (np.cos(X) + 2) / (np.cos(X * 1.4) + 2)
    noise = np.random.normal(0, 0.2, N)
    y = ytrue + noise
    return X, y, ytrue
  
x, y, ytrue = genDataSet(100)
plt.plot(x,y,'.')
plt.plot(x,ytrue,'rx')
plt.show()

kf = KFold(n_splits=10)

bestkall = []
for i in range(100):
  x, y, ytrue = genDataSet(100)
  X = np.array([x,y]).T
  kf.get_n_splits(X)

  msek = {}
  #plt.figure(figsize=(10,10))
  bestk = 0
  bestmse = 100000
  for k in np.arange(1,2*np.floor(((len(ytrue)*0.9)+1)/2),2):
    #print(k)
    mse = []
    for train_index, test_index in kf.split(X):
      X_train, X_test =     X[train_index],     X[test_index]
      y_train, y_test = ytrue[train_index], ytrue[test_index]


      neigh = KNeighborsRegressor(n_neighbors=int(k))
      neigh.fit(X_train, y_train)
      predictions = neigh.predict(X_test)

      mse.append(mean_squared_error(y_test, predictions))

    #print(np.mean(mse))
    msek[k] = np.mean(mse)
    if bestmse > msek[k]:
      bestmse = msek[k]
      bestk = k
      print(bestk,bestmse)

  bestkall.append(bestk)
  #plt.plot(k,msek[k],'r.')
  
#plt.show()
print(bestkall)
plt.hist(bestkall,int(np.max(bestkall)))